{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Step 1 - Import modules."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\steph\\AppData\\Local\\Temp\\ipykernel_14908\\442179322.py:1: DtypeWarning: Columns (0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  lab_data = pd.read_csv('../data/lab_data_cleaned.csv')\n"
     ]
    }
   ],
   "source": [
    "lab_data = pd.read_csv('../data/lab_data_cleaned.csv')\n",
    "admissions = pd.read_csv('../data/admissions_cleaned.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "  Unnamed: 0            Unnamed: 1                     Value  Value.1  \\\n0        Lab                   NaN  Alanine aminotransferase  Albumin   \n1    Patient                  Time                       NaN      NaN   \n2          3  0.002777777777777778                       NaN      NaN   \n3          3  0.004166666666666667                       NaN      NaN   \n4          3                0.0125                       NaN      NaN   \n\n             Value.2            Value.3          Value.4             Value.5  \\\n0  Albumin (ascites)  Albumin (pleural)  Albumin (urine)  Alkaline phosphate   \n1                NaN                NaN              NaN                 NaN   \n2                NaN                NaN              NaN                 NaN   \n3                NaN                NaN              NaN                 NaN   \n4                NaN                NaN              NaN                 NaN   \n\n     Value.6                    Value.7  ...                        Value.57  \\\n0  Anion gap  Asparate aminotransferase  ...  Red blood cell count (ascites)   \n1        NaN                        NaN  ...                             NaN   \n2        NaN                        NaN  ...                             NaN   \n3        NaN                        NaN  ...                             NaN   \n4       23.0                        NaN  ...                             NaN   \n\n                         Value.58 Value.59              Value.60  \\\n0  Red blood cell count (pleural)   Sodium  Sodium (whole blood)   \n1                             NaN      NaN                   NaN   \n2                             NaN      NaN                 138.0   \n3                             NaN      NaN                 153.0   \n4                             NaN    143.0                   NaN   \n\n                Value.61             Value.62    Value.63    Value.64  \\\n0  Tidal Volume Observed  Total Protein Urine  Troponin-I  Troponin-T   \n1                    NaN                  NaN         NaN         NaN   \n2                    NaN                  NaN         NaN         NaN   \n3                    NaN                  NaN         NaN         NaN   \n4                    NaN                  NaN         NaN         NaN   \n\n                 Value.65 Value.66  \n0  White blood cell count       pH  \n1                     NaN      NaN  \n2                     NaN     7.35  \n3                     NaN     7.59  \n4                    11.3      NaN  \n\n[5 rows x 69 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>Unnamed: 1</th>\n      <th>Value</th>\n      <th>Value.1</th>\n      <th>Value.2</th>\n      <th>Value.3</th>\n      <th>Value.4</th>\n      <th>Value.5</th>\n      <th>Value.6</th>\n      <th>Value.7</th>\n      <th>...</th>\n      <th>Value.57</th>\n      <th>Value.58</th>\n      <th>Value.59</th>\n      <th>Value.60</th>\n      <th>Value.61</th>\n      <th>Value.62</th>\n      <th>Value.63</th>\n      <th>Value.64</th>\n      <th>Value.65</th>\n      <th>Value.66</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Lab</td>\n      <td>NaN</td>\n      <td>Alanine aminotransferase</td>\n      <td>Albumin</td>\n      <td>Albumin (ascites)</td>\n      <td>Albumin (pleural)</td>\n      <td>Albumin (urine)</td>\n      <td>Alkaline phosphate</td>\n      <td>Anion gap</td>\n      <td>Asparate aminotransferase</td>\n      <td>...</td>\n      <td>Red blood cell count (ascites)</td>\n      <td>Red blood cell count (pleural)</td>\n      <td>Sodium</td>\n      <td>Sodium (whole blood)</td>\n      <td>Tidal Volume Observed</td>\n      <td>Total Protein Urine</td>\n      <td>Troponin-I</td>\n      <td>Troponin-T</td>\n      <td>White blood cell count</td>\n      <td>pH</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Patient</td>\n      <td>Time</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>0.002777777777777778</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>138.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>7.35</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>0.004166666666666667</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>153.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>7.59</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3</td>\n      <td>0.0125</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>23.0</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>143.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>11.3</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 69 columns</p>\n</div>"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lab_data.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "   Patient  HADM_ID            ADMITTIME            DISCHTIME  \\\n0        3   145834  2101-10-20 19:08:00  2101-10-31 13:58:00   \n1        4   185777  2191-03-16 00:28:00  2191-03-23 18:41:00   \n2        6   107064  2175-05-30 07:15:00  2175-06-15 16:00:00   \n3        9   150750  2149-11-09 13:06:00  2149-11-14 10:15:00   \n4       11   194540  2178-04-16 06:18:00  2178-05-11 19:00:00   \n\n             DEATHTIME ADMISSION_TYPE INSURANCE              ETHNICITY  \\\n0                  NaN      EMERGENCY  Medicare                  WHITE   \n1                  NaN      EMERGENCY   Private                  WHITE   \n2                  NaN       ELECTIVE  Medicare                  WHITE   \n3  2149-11-14 10:15:00      EMERGENCY  Medicaid  UNKNOWN/NOT SPECIFIED   \n4                  NaN      EMERGENCY   Private                  WHITE   \n\n                             DIAGNOSIS  HOSPITAL_EXPIRE_FLAG GENDER  \\\n0                          HYPOTENSION                     0      M   \n1  FEVER,DEHYDRATION,FAILURE TO THRIVE                     0      F   \n2            CHRONIC RENAL FAILURE/SDA                     0      F   \n3                      HEMORRHAGIC CVA                     1      M   \n4                           BRAIN MASS                     0      F   \n\n          DOB        AGE  Death               LOS  \n0  2025-04-11  76.575342  False  10 days 18:50:00  \n1  2143-05-12  47.876712  False   7 days 18:13:00  \n2  2109-06-21  65.983562  False  16 days 08:45:00  \n3  2108-01-26  41.816438   True   4 days 21:09:00  \n4  2128-02-22  50.180822  False  25 days 12:42:00  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Patient</th>\n      <th>HADM_ID</th>\n      <th>ADMITTIME</th>\n      <th>DISCHTIME</th>\n      <th>DEATHTIME</th>\n      <th>ADMISSION_TYPE</th>\n      <th>INSURANCE</th>\n      <th>ETHNICITY</th>\n      <th>DIAGNOSIS</th>\n      <th>HOSPITAL_EXPIRE_FLAG</th>\n      <th>GENDER</th>\n      <th>DOB</th>\n      <th>AGE</th>\n      <th>Death</th>\n      <th>LOS</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3</td>\n      <td>145834</td>\n      <td>2101-10-20 19:08:00</td>\n      <td>2101-10-31 13:58:00</td>\n      <td>NaN</td>\n      <td>EMERGENCY</td>\n      <td>Medicare</td>\n      <td>WHITE</td>\n      <td>HYPOTENSION</td>\n      <td>0</td>\n      <td>M</td>\n      <td>2025-04-11</td>\n      <td>76.575342</td>\n      <td>False</td>\n      <td>10 days 18:50:00</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>185777</td>\n      <td>2191-03-16 00:28:00</td>\n      <td>2191-03-23 18:41:00</td>\n      <td>NaN</td>\n      <td>EMERGENCY</td>\n      <td>Private</td>\n      <td>WHITE</td>\n      <td>FEVER,DEHYDRATION,FAILURE TO THRIVE</td>\n      <td>0</td>\n      <td>F</td>\n      <td>2143-05-12</td>\n      <td>47.876712</td>\n      <td>False</td>\n      <td>7 days 18:13:00</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>6</td>\n      <td>107064</td>\n      <td>2175-05-30 07:15:00</td>\n      <td>2175-06-15 16:00:00</td>\n      <td>NaN</td>\n      <td>ELECTIVE</td>\n      <td>Medicare</td>\n      <td>WHITE</td>\n      <td>CHRONIC RENAL FAILURE/SDA</td>\n      <td>0</td>\n      <td>F</td>\n      <td>2109-06-21</td>\n      <td>65.983562</td>\n      <td>False</td>\n      <td>16 days 08:45:00</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9</td>\n      <td>150750</td>\n      <td>2149-11-09 13:06:00</td>\n      <td>2149-11-14 10:15:00</td>\n      <td>2149-11-14 10:15:00</td>\n      <td>EMERGENCY</td>\n      <td>Medicaid</td>\n      <td>UNKNOWN/NOT SPECIFIED</td>\n      <td>HEMORRHAGIC CVA</td>\n      <td>1</td>\n      <td>M</td>\n      <td>2108-01-26</td>\n      <td>41.816438</td>\n      <td>True</td>\n      <td>4 days 21:09:00</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>11</td>\n      <td>194540</td>\n      <td>2178-04-16 06:18:00</td>\n      <td>2178-05-11 19:00:00</td>\n      <td>NaN</td>\n      <td>EMERGENCY</td>\n      <td>Private</td>\n      <td>WHITE</td>\n      <td>BRAIN MASS</td>\n      <td>0</td>\n      <td>F</td>\n      <td>2128-02-22</td>\n      <td>50.180822</td>\n      <td>False</td>\n      <td>25 days 12:42:00</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "admissions['Death'] = admissions['Death'] < 8\n",
    "admissions.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\steph\\AppData\\Local\\Temp\\ipykernel_14908\\2088363609.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  groups.ETHNICITY = admissions.ETHNICITY.str.contains('BLACK')\n",
      "C:\\Users\\steph\\AppData\\Local\\Temp\\ipykernel_14908\\2088363609.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  groups.GENDER = (admissions.ETHNICITY == 'M')\n",
      "C:\\Users\\steph\\AppData\\Local\\Temp\\ipykernel_14908\\2088363609.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  groups.INSURANCE = (admissions.INSURANCE == 'Private')\n"
     ]
    }
   ],
   "source": [
    "groups = admissions[['ETHNICITY', 'GENDER', 'INSURANCE']]\n",
    "groups.ETHNICITY = admissions.ETHNICITY.str.contains('BLACK')\n",
    "groups.GENDER = (admissions.ETHNICITY == 'M')\n",
    "groups.INSURANCE = (admissions.INSURANCE == 'Private')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total patients: 36296\n",
      "Training patients: 29037\n"
     ]
    }
   ],
   "source": [
    "results = 'results/classification'\n",
    "training = pd.Series(admissions.index.isin(admissions.sample(frac=0.8, random_state = 0).index), index=admissions.index)\n",
    "print(f'Total patients: {len(training)}')\n",
    "print(f'Training patients: {training.sum()}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "def imputation(train_index, data, groups, strategy = 'Median', add_count = False, add_group = False, max_iter = 10):\n",
    "    imputed = data.add_suffix('_data').groupby('Patient').last()\n",
    "\n",
    "    if add_count:\n",
    "        # Add count of observed test\n",
    "        imputed = pd.concat([imputed, (imputed.isna()).add_suffix('_count')], axis = 1)\n",
    "\n",
    "    if add_group:\n",
    "        # Add group\n",
    "        imputed = imputed.join(groups.add_suffix('_group'))\n",
    "\n",
    "    if 'Group' in strategy:\n",
    "        # Add group befoer splitting only for imputation\n",
    "        imputed = imputed.join(groups.add_suffix('_group_reg'))\n",
    "\n",
    "    # Data to use to learn imputation\n",
    "    train_data = imputed.loc[imputed.index.get_level_values('Patient').isin(train_index)]\n",
    "    train_index = train_data.index\n",
    "\n",
    "    # Compute fill value\n",
    "    if strategy == 'LOCF':\n",
    "        imputed = imputed.groupby('Patient').ffill()\n",
    "        impute = - 1\n",
    "\n",
    "    if strategy == 'Individual':\n",
    "        impute = imputed.groupby('Patient').median()\n",
    "\n",
    "    if strategy == 'Median':\n",
    "        impute = train_data.median()\n",
    "\n",
    "    if strategy == 'Mean':\n",
    "        impute = train_data.mean()\n",
    "\n",
    "    if strategy == 'Group Median':\n",
    "        impute = train_data.groupby(groups).transform(lambda x: x.fillna(x.median()))\n",
    "\n",
    "    if strategy == 'Group Mean':\n",
    "        impute = train_data.groupby(groups).transform(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "    if 'MICE' in strategy:\n",
    "        impute = -1\n",
    "\n",
    "        # MICE Algorithm\n",
    "        ## 1. Init with median imputation\n",
    "        missing = imputed.isna()\n",
    "        imputed = pd.DataFrame(SimpleImputer(strategy = \"median\").fit(train_data.values).transform(imputed.values), index = imputed.index, columns = imputed.columns)\n",
    "\n",
    "        ## 2. Iterate through columns\n",
    "        ### Find columns with random values (start with the one with least)\n",
    "        to_impute = missing.sum().sort_values()\n",
    "        to_impute = to_impute[to_impute > 0]\n",
    "\n",
    "        ### Impute one by one with regression until convergence\n",
    "        for _ in range(max_iter):\n",
    "            for c in to_impute.index:\n",
    "                #### Take train points for which c is observed to train model\n",
    "                train_data = imputed.loc[train_index][~missing.loc[train_index][c]]\n",
    "\n",
    "                #### Fit regression\n",
    "                lr = LinearRegression().fit(train_data.loc[:, imputed.columns != c].values, train_data[c].values)\n",
    "                residuals = np.abs(lr.predict(train_data.loc[:, imputed.columns != c].values) - train_data[c])\n",
    "\n",
    "                #### Draw with normal error\n",
    "                prev = imputed.copy()\n",
    "                imputed[c][missing[c]] = lr.predict(imputed.loc[:, imputed.columns != c][missing[c]].values) + np.random.normal(scale = np.std(residuals), size = missing[c].sum())\n",
    "        else:\n",
    "            if 'Group' in strategy:\n",
    "                # Remove the group columns of imputed data\n",
    "                imputed = imputed.iloc[:, :-1]\n",
    "\n",
    "    return imputed, impute\n",
    "\n",
    "\n",
    "def process(train_index, data, groups, **args):\n",
    "    \"\"\"\n",
    "        Preprocesses data\n",
    "        Take last observation and impute given strategy\n",
    "    \"\"\"\n",
    "    updated, impute = imputation(train_index, data, groups, **args)\n",
    "    #resampled = updated.groupby('Patient').last()\n",
    "    imputed = updated.fillna(impute)\n",
    "\n",
    "    return imputed"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "from mimic_utils import Experiment"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    'penalty': ['l2'],\n",
    "    'C': [0.01, 0.1, 1., 10],\n",
    "    'solver': ['sag'],\n",
    "    'max_iter': [1000],\n",
    "    'n_jobs': [-1]\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "imputations = {\n",
    "                #'Median': {'strategy': 'Median'},\n",
    "                'Median Missing': {'strategy': 'Median', 'add_count': True},\n",
    "                #'MICE': {'strategy': 'MICE', 'n_iter': 10},\n",
    "                'MICE Missing': {'strategy': 'MICE', 'n_iter': 10, 'add_count': True},\n",
    "                #'Group MICE': {'strategy': 'Group MICE', 'n_iter': 10},\n",
    "                #'Group MICE Missing': {'strategy': 'Group MICE', 'n_iter': 10, 'add_count': True},\n",
    "                #'Individual': {'strategy': 'Individual'},\n",
    "                #'LOCF': {'strategy': 'LOCF'},\n",
    "                #'LOCF Count': {'strategy': 'LOCF', 'add_count': True},\n",
    "                #'LOCF Group': {'strategy': 'LOCF', 'add_count': True, 'add_group': True},\n",
    "              }"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputation strategy:  Median Missing\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Patient'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[16], line 7\u001B[0m\n\u001B[0;32m      5\u001B[0m predictions \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m \u001B[38;5;28miter\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(n_iter):\n\u001B[1;32m----> 7\u001B[0m     last \u001B[38;5;241m=\u001B[39m process(training[training]\u001B[38;5;241m.\u001B[39mindex, lab_data, groups, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams)\n\u001B[0;32m      8\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m (last \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39msum()\u001B[38;5;241m.\u001B[39msum() \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNon imputed values\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     10\u001B[0m     se \u001B[38;5;241m=\u001B[39m Experiment\u001B[38;5;241m.\u001B[39mcreate(model \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlog\u001B[39m\u001B[38;5;124m'\u001B[39m, hyper_grid \u001B[38;5;241m=\u001B[39m hyperparams, save \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m, path \u001B[38;5;241m=\u001B[39m results \u001B[38;5;241m+\u001B[39m name)\n",
      "Cell \u001B[1;32mIn[9], line 83\u001B[0m, in \u001B[0;36mprocess\u001B[1;34m(train_index, data, groups, **args)\u001B[0m\n\u001B[0;32m     78\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mprocess\u001B[39m(train_index, data, groups, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39margs):\n\u001B[0;32m     79\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     80\u001B[0m \u001B[38;5;124;03m        Preprocesses data \u001B[39;00m\n\u001B[0;32m     81\u001B[0m \u001B[38;5;124;03m        Take last observation and impute given strategy\u001B[39;00m\n\u001B[0;32m     82\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m---> 83\u001B[0m     updated, impute \u001B[38;5;241m=\u001B[39m imputation(train_index, data, groups, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39margs)\n\u001B[0;32m     84\u001B[0m     \u001B[38;5;66;03m#resampled = updated.groupby('Patient').last()\u001B[39;00m\n\u001B[0;32m     85\u001B[0m     imputed \u001B[38;5;241m=\u001B[39m updated\u001B[38;5;241m.\u001B[39mfillna(impute)\n",
      "Cell \u001B[1;32mIn[9], line 6\u001B[0m, in \u001B[0;36mimputation\u001B[1;34m(train_index, data, groups, strategy, add_count, add_group, max_iter)\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mimputation\u001B[39m(train_index, data, groups, strategy \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mMedian\u001B[39m\u001B[38;5;124m'\u001B[39m, add_count \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m, add_group \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m, max_iter \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m10\u001B[39m):\n\u001B[1;32m----> 6\u001B[0m     imputed \u001B[38;5;241m=\u001B[39m \u001B[43mdata\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madd_suffix\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m_data\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgroupby\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mPatient\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mlast()\n\u001B[0;32m      8\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m add_count:\n\u001B[0;32m      9\u001B[0m         \u001B[38;5;66;03m# Add count of observed test\u001B[39;00m\n\u001B[0;32m     10\u001B[0m         imputed \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mconcat([imputed, (imputed\u001B[38;5;241m.\u001B[39misna())\u001B[38;5;241m.\u001B[39madd_suffix(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m_count\u001B[39m\u001B[38;5;124m'\u001B[39m)], axis \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[1;32m~\\OneDrive\\Documents\\Georgia Tech\\ISYE 8803 - High Dimensional Data Analytics\\HW1\\lib\\site-packages\\pandas\\core\\frame.py:8389\u001B[0m, in \u001B[0;36mDataFrame.groupby\u001B[1;34m(self, by, axis, level, as_index, sort, group_keys, squeeze, observed, dropna)\u001B[0m\n\u001B[0;32m   8386\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou have to supply one of \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mby\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m and \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlevel\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   8387\u001B[0m axis \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_axis_number(axis)\n\u001B[1;32m-> 8389\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mDataFrameGroupBy\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   8390\u001B[0m \u001B[43m    \u001B[49m\u001B[43mobj\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m   8391\u001B[0m \u001B[43m    \u001B[49m\u001B[43mkeys\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mby\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   8392\u001B[0m \u001B[43m    \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   8393\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlevel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlevel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   8394\u001B[0m \u001B[43m    \u001B[49m\u001B[43mas_index\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mas_index\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   8395\u001B[0m \u001B[43m    \u001B[49m\u001B[43msort\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msort\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   8396\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgroup_keys\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup_keys\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   8397\u001B[0m \u001B[43m    \u001B[49m\u001B[43msqueeze\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msqueeze\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   8398\u001B[0m \u001B[43m    \u001B[49m\u001B[43mobserved\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mobserved\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   8399\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdropna\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdropna\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   8400\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\OneDrive\\Documents\\Georgia Tech\\ISYE 8803 - High Dimensional Data Analytics\\HW1\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py:959\u001B[0m, in \u001B[0;36mGroupBy.__init__\u001B[1;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, squeeze, observed, mutated, dropna)\u001B[0m\n\u001B[0;32m    956\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m grouper \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    957\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mgroupby\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mgrouper\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m get_grouper\n\u001B[1;32m--> 959\u001B[0m     grouper, exclusions, obj \u001B[38;5;241m=\u001B[39m \u001B[43mget_grouper\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    960\u001B[0m \u001B[43m        \u001B[49m\u001B[43mobj\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    961\u001B[0m \u001B[43m        \u001B[49m\u001B[43mkeys\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    962\u001B[0m \u001B[43m        \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    963\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlevel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlevel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    964\u001B[0m \u001B[43m        \u001B[49m\u001B[43msort\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msort\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    965\u001B[0m \u001B[43m        \u001B[49m\u001B[43mobserved\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mobserved\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    966\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmutated\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmutated\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    967\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdropna\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdropna\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    968\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    970\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobj \u001B[38;5;241m=\u001B[39m obj\n\u001B[0;32m    971\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maxis \u001B[38;5;241m=\u001B[39m obj\u001B[38;5;241m.\u001B[39m_get_axis_number(axis)\n",
      "File \u001B[1;32m~\\OneDrive\\Documents\\Georgia Tech\\ISYE 8803 - High Dimensional Data Analytics\\HW1\\lib\\site-packages\\pandas\\core\\groupby\\grouper.py:888\u001B[0m, in \u001B[0;36mget_grouper\u001B[1;34m(obj, key, axis, level, sort, observed, mutated, validate, dropna)\u001B[0m\n\u001B[0;32m    886\u001B[0m         in_axis, level, gpr \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m, gpr, \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    887\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 888\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(gpr)\n\u001B[0;32m    889\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(gpr, Grouper) \u001B[38;5;129;01mand\u001B[39;00m gpr\u001B[38;5;241m.\u001B[39mkey \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    890\u001B[0m     \u001B[38;5;66;03m# Add key to exclusions\u001B[39;00m\n\u001B[0;32m    891\u001B[0m     exclusions\u001B[38;5;241m.\u001B[39madd(gpr\u001B[38;5;241m.\u001B[39mkey)\n",
      "\u001B[1;31mKeyError\u001B[0m: 'Patient'"
     ]
    }
   ],
   "source": [
    "for name, params in imputations.items():\n",
    "    print('Imputation strategy: ', name)\n",
    "    n_iter = params.pop('n_iter', 1)\n",
    "\n",
    "    predictions = []\n",
    "    for iter in range(n_iter):\n",
    "        last = process(training[training].index, lab_data, groups, **params)\n",
    "        assert (last == -1).sum().sum() == 0, \"Non imputed values\"\n",
    "\n",
    "        se = Experiment.create(model = 'log', hyper_grid = hyperparams, save = False, path = results + name)\n",
    "        pred = se.train(last, admissions.Death, training)\n",
    "        if pred is None: break # Reload previous copy\n",
    "        predictions.append(pred)\n",
    "    else:\n",
    "        # Average Multiple imputations models\n",
    "        used = [p.Use for p in predictions][-1]\n",
    "        predictions = pd.concat([p[1] for p in predictions], axis = 1)\n",
    "        predictions = pd.concat({'Mean': predictions.mean(1), 'Std': predictions.std(1)}, axis = 1)\n",
    "        se = Experiment.create(model = 'log', hyper_grid = hyperparams, path = results + name)\n",
    "        se.save_results(predictions, used)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}